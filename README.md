# Классификация бабочек

Этот проект направлен на автоматическую классификацию изображений бабочек по 50 различным видам с использованием нейронных сетей. Задача взята с соревнования на Kaggle:
[https://www.kaggle.com/competitions/butterflies-classification/](https://www.kaggle.com/competitions/butterflies-classification/)

## Описание данных

* **train/** — изображения бабочек с метками классов от 0 до 49 (4955 изображений, размер 224*224)
* **test/** — изображения бабочек без меток (250 изображений)

## Используемая модель

Модель построена с использованием PyTorch (файл nn-butterflies.ipynb). В ходе экспериментов были реализованы разные подходы:

* Начальное обучение с нуля (simple CNN), которое дало ограниченную точность и оказалось затратным по ресурсам
* Использование предобученных моделей (transfer learning) с архитектурами ResNet, EfficientNet и VGG
* Лучшая модель — **EfficientNet-B3** с дообучением всех слоёв и адаптацией выходного слоя под 50 классов
* Предобработка изображений:

  * Resize до 224x224
  * Normalization с использованием средних значений и стандартных отклонений ImageNet
  * Аугментации: случайное горизонтальное отражение, изменение яркости, контрастности, повороты и др.

* Обучение на GPU (CUDA), кросс-валидация с train/val split
* Классификатор: Dropout + Linear Layer
* Функция потерь: `CrossEntropyLoss`
* Оптимизатор: `Adam` с подобранными гиперпараметрами

Также добавлен файл butterflies-ensemble.ipynb, содержащий эксперимент с ансамблированием нескольких моделей.

## Запуск

### Установка зависимостей

Создайте и активируйте виртуальное окружение, затем установите зависимости:

```bash
pip install torch torchvision
pip install matplotlib pandas scikit-learn
pip install tqdm albumentations timm
```

### Запуск обучения и инференса

```bash
jupyter notebook nn-butterflies.ipynb
```

В ноутбуке реализованы:

* Подгрузка и визуализация данных
* Обработка и аугментация изображений
* Обучение нескольких моделей
* Валидация и графики обучения
* Предсказания на тестовой выборке и формирование `submission.csv`

## Результаты и выводы

1. Очевидно, вторая модель оказалась лучше первой как на обучающей, так и на валидационной выборках по loss и accuracy. Третья модель, EfficientNet B3, показала ещё более высокие результаты, как в плане точности, так и устойчивости к переобучению.

2. Обучение модели с нуля оказалось медленным, ресурсоёмким и менее эффективным. Transfer learning с предобученными весами дал значительно лучший результат при меньших затратах.

3. Важную роль сыграли аугментации, изменение параметров оптимизатора и использование большего количества тренировочных данных. Это позволило достичь высокой точности на валидационном наборе данных.

4. Не все способы улучшения точности оказались эффективными. Было протестировано множество подходов:

   * **4.1 Предобученные модели**: EfficientNet B3 показала лучшие результаты среди B3, B4, B5, ResNet50, Wide ResNet50, VGG17 и других. Более глубокие архитектуры (например, B5) переобучались или не давали значимого прироста точности.

   * **4.2 Архитектуры и обучение**: Заморозка весов (полная или частичная) при дообучении не дала прироста в accuracy. Это может быть связано с тем, что классы бабочек визуально очень похожи. Необходим детальный анализ мелких деталей, таких как форма пятен и узоры на крыльях.

   * **4.3 Оптимизация**: Регуляризация с помощью weight decay, планировщики learning rate и прочие тонкие настройки не дали ощутимого прироста. Оптимальный результат достигался с базовой настройкой Adam без scheduler.

   * **4.4 Аугментации**: Были подобраны наиболее эффективные трансформации, в частности горизонтальное отражение, повороты, искажения и изменения цвета. Они улучшили обобщающую способность модели.

   * **4.5 Ансамблирование**: Попытки объединения нескольких моделей не привели к улучшению результатов. Лучший результат достигался одной хорошо обученной моделью.

## Заключение

Модель классификации бабочек достигла высокой точности (0.97340) благодаря использованию предобученных EfficientNet-моделей и тщательному подбору стратегий обучения. Проект демонстрирует эффективность transfer learning для задач с ограниченным количеством данных и высоким межклассовым сходством. Следующими шагами могут быть более продвинутая аугментация, улучшение стратегии кросс-валидации и использование техник semi-supervised learning.

